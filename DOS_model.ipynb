{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union, Tuple\n",
    "from math import ceil, sqrt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import package\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.norm import LayerNorm, PairNorm, InstanceNorm\n",
    "from torch_geometric.typing import PairTensor, Adj, OptTensor, Size\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.models.dimenet import BesselBasisLayer\n",
    "from torch_scatter import scatter_add, scatter\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from deeph.data import HData\n",
    "\n",
    "from deeph.from_se3_transformer import SphericalHarmonics\n",
    "from deeph.from_schnetpack import GaussianBasis\n",
    "from deeph.from_PyG_future import GraphNorm, DiffGroupNorm\n",
    "from deeph.from_HermNet import RBF, cosine_cutoff, ShiftedSoftplus, _eps\n",
    "from inspect import signature\n",
    "\n",
    "from deeph.utils import LossRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGConv(MessagePassing):\n",
    "    def __init__(self, channels: Union[int, Tuple[int, int]], dim: int = 0,\n",
    "                 aggr: str = 'add', normalization: str = None,\n",
    "                 bias: bool = True, if_exp: bool = False, **kwargs):\n",
    "        super(CGConv, self).__init__(aggr=aggr, flow=\"source_to_target\", **kwargs)\n",
    "        self.channels = channels\n",
    "        self.dim = dim\n",
    "        self.normalization = normalization\n",
    "        self.if_exp = if_exp\n",
    "\n",
    "        if isinstance(channels, int):\n",
    "            channels = (channels, channels)\n",
    "\n",
    "        self.lin_f = nn.Linear(sum(channels) + dim, channels[1], bias=bias)\n",
    "        self.lin_s = nn.Linear(sum(channels) + dim, channels[1], bias=bias)\n",
    "        if self.normalization == 'BatchNorm':\n",
    "            self.bn = nn.BatchNorm1d(channels[1], track_running_stats=True)\n",
    "        elif self.normalization == 'LayerNorm':\n",
    "            self.ln = LayerNorm(channels[1])\n",
    "        elif self.normalization == 'PairNorm':\n",
    "            self.pn = PairNorm(channels[1])\n",
    "        elif self.normalization == 'InstanceNorm':\n",
    "            self.instance_norm = InstanceNorm(channels[1])\n",
    "        elif self.normalization == 'GraphNorm':\n",
    "            self.gn = GraphNorm(channels[1])\n",
    "        elif self.normalization == 'DiffGroupNorm':\n",
    "            self.group_norm = DiffGroupNorm(channels[1], 128)\n",
    "        elif self.normalization is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Unknown normalization function: {}'.format(normalization))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_f.reset_parameters()\n",
    "        self.lin_s.reset_parameters()\n",
    "        if self.normalization == 'BatchNorm':\n",
    "            self.bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Union[torch.Tensor, PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor, batch, distance, size: Size = None) -> torch.Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x: PairTensor = (x, x)\n",
    "\n",
    "        # propagate_type: (x: PairTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, distance=distance, size=size)\n",
    "        if self.normalization == 'BatchNorm':\n",
    "            out = self.bn(out)\n",
    "        elif self.normalization == 'LayerNorm':\n",
    "            out = self.ln(out, batch)\n",
    "        elif self.normalization == 'PairNorm':\n",
    "            out = self.pn(out, batch)\n",
    "        elif self.normalization == 'InstanceNorm':\n",
    "            out = self.instance_norm(out, batch)\n",
    "        elif self.normalization == 'GraphNorm':\n",
    "            out = self.gn(out, batch)\n",
    "        elif self.normalization == 'DiffGroupNorm':\n",
    "            out = self.group_norm(out)\n",
    "        out += x[1]\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr: OptTensor, distance) -> torch.Tensor:\n",
    "        z = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
    "        out = self.lin_f(z).sigmoid() * F.softplus(self.lin_s(z))\n",
    "        if self.if_exp: # Very specific section. The message is scaled exponentially with the distance.\n",
    "            sigma = 3\n",
    "            n = 2\n",
    "            out = out * torch.exp(-distance ** n / sigma ** n / 2).view(-1, 1)\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, dim={})'.format(self.__class__.__name__, self.channels, self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPLayer(nn.Module):\n",
    "    def __init__(self, in_atom_fea_len, in_edge_fea_len, out_edge_fea_len, if_exp, if_edge_update, normalization,\n",
    "                 atom_update_net, gauss_stop, output_layer=False):\n",
    "        super(MPLayer, self).__init__()\n",
    "        if atom_update_net == 'CGConv':\n",
    "            self.cgconv = CGConv(channels=in_atom_fea_len,\n",
    "                                 dim=in_edge_fea_len,\n",
    "                                 aggr='add',\n",
    "                                 normalization=normalization,\n",
    "                                 if_exp=if_exp)\n",
    "\n",
    "        self.if_edge_update = if_edge_update\n",
    "        self.atom_update_net = atom_update_net\n",
    "        if if_edge_update:\n",
    "            if output_layer:\n",
    "                self.e_lin = nn.Sequential(nn.Linear(in_edge_fea_len + in_atom_fea_len * 2, 128),\n",
    "                                           nn.SiLU(),\n",
    "                                           nn.Linear(128, out_edge_fea_len),\n",
    "                                           )\n",
    "            else:\n",
    "                self.e_lin = nn.Sequential(nn.Linear(in_edge_fea_len + in_atom_fea_len * 2, 128),\n",
    "                                           nn.SiLU(),\n",
    "                                           nn.Linear(128, out_edge_fea_len),\n",
    "                                           nn.SiLU(),\n",
    "                                           )\n",
    "\n",
    "    def forward(self, atom_fea, edge_idx, edge_fea, batch, distance, edge_vec):\n",
    "        atom_fea = self.cgconv(atom_fea, edge_idx, edge_fea, batch, distance)\n",
    "        atom_fea_s = atom_fea\n",
    "        if self.if_edge_update:\n",
    "            row, col = edge_idx\n",
    "            edge_fea = self.e_lin(torch.cat([atom_fea_s[row], atom_fea_s[col], edge_fea], dim=-1))\n",
    "            return atom_fea, edge_fea\n",
    "        else:\n",
    "            return atom_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCMPLayer(nn.Module):\n",
    "    def __init__(self, in_atom_fea_len, in_edge_fea_len, out_edge_fea_len, num_l,\n",
    "                 normalization: str = None, bias: bool = True, if_exp: bool = False):\n",
    "        super(LCMPLayer, self).__init__()\n",
    "        self.in_atom_fea_len = in_atom_fea_len\n",
    "        self.normalization = normalization\n",
    "        self.if_exp = if_exp\n",
    "\n",
    "        self.lin_f = nn.Linear(in_atom_fea_len * 2 + in_edge_fea_len, in_atom_fea_len, bias=bias)\n",
    "        self.lin_s = nn.Linear(in_atom_fea_len * 2 + in_edge_fea_len, in_atom_fea_len, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(in_atom_fea_len, track_running_stats=True)\n",
    "\n",
    "        self.e_lin = nn.Sequential(nn.Linear(in_edge_fea_len + in_atom_fea_len * 2 - num_l ** 2, 128),\n",
    "                                   nn.SiLU(),\n",
    "                                   nn.Linear(128, out_edge_fea_len)\n",
    "                                   )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_f.reset_parameters()\n",
    "        self.lin_s.reset_parameters()\n",
    "        if self.normalization == 'BatchNorm':\n",
    "            self.bn.reset_parameters()\n",
    "\n",
    "    def forward(self, atom_fea, edge_fea, sub_atom_idx, sub_edge_idx, sub_edge_ang, sub_index, distance,\n",
    "                huge_structure, output_final_layer_neuron):\n",
    "        if huge_structure:\n",
    "            sub_graph_batch_num = 8\n",
    "\n",
    "            sub_graph_num = sub_atom_idx.shape[0]\n",
    "            sub_graph_batch_size = ceil(sub_graph_num / sub_graph_batch_num)\n",
    "\n",
    "            num_edge = edge_fea.shape[0]\n",
    "            vf_update = torch.zeros((num_edge * 2, self.in_atom_fea_len)).type(torch.get_default_dtype()).to(atom_fea.device)\n",
    "            for sub_graph_batch_index in range(sub_graph_batch_num):\n",
    "                if sub_graph_batch_index == sub_graph_batch_num - 1:\n",
    "                    sub_graph_idx = slice(sub_graph_batch_size * sub_graph_batch_index, sub_graph_num)\n",
    "                else:\n",
    "                    sub_graph_idx = slice(sub_graph_batch_size * sub_graph_batch_index,\n",
    "                                          sub_graph_batch_size * (sub_graph_batch_index + 1))\n",
    "\n",
    "                sub_atom_idx_batch = sub_atom_idx[sub_graph_idx]\n",
    "                sub_edge_idx_batch = sub_edge_idx[sub_graph_idx]\n",
    "                sub_edge_ang_batch = sub_edge_ang[sub_graph_idx]\n",
    "                sub_index_batch = sub_index[sub_graph_idx]\n",
    "\n",
    "                z = torch.cat([atom_fea[sub_atom_idx_batch][:, 0, :], atom_fea[sub_atom_idx_batch][:, 1, :],\n",
    "                               edge_fea[sub_edge_idx_batch], sub_edge_ang_batch], dim=-1)\n",
    "                out = self.lin_f(z).sigmoid() * F.softplus(self.lin_s(z))\n",
    "\n",
    "                if self.if_exp:\n",
    "                    sigma = 3\n",
    "                    n = 2\n",
    "                    out = out * torch.exp(-distance[sub_edge_idx_batch] ** n / sigma ** n / 2).view(-1, 1)\n",
    "\n",
    "                vf_update += scatter_add(out, sub_index_batch, dim=0, dim_size=num_edge * 2)\n",
    "\n",
    "            if self.normalization == 'BatchNorm':\n",
    "                vf_update = self.bn(vf_update)\n",
    "            vf_update = vf_update.reshape(num_edge, 2, -1)\n",
    "            if output_final_layer_neuron != '':\n",
    "                final_layer_neuron = torch.cat([vf_update[:, 0, :], vf_update[:, 1, :], edge_fea],\n",
    "                                               dim=-1).detach().cpu().numpy()\n",
    "                np.save(os.path.join(output_final_layer_neuron, 'final_layer_neuron.npy'), final_layer_neuron)\n",
    "            out = self.e_lin(torch.cat([vf_update[:, 0, :], vf_update[:, 1, :], edge_fea], dim=-1))\n",
    "\n",
    "            return out\n",
    "\n",
    "        num_edge = edge_fea.shape[0]\n",
    "        z = torch.cat(\n",
    "            [atom_fea[sub_atom_idx][:, 0, :], atom_fea[sub_atom_idx][:, 1, :], edge_fea[sub_edge_idx], sub_edge_ang],\n",
    "            dim=-1)\n",
    "        out = self.lin_f(z).sigmoid() * F.softplus(self.lin_s(z))\n",
    "\n",
    "        if self.if_exp:\n",
    "            sigma = 3\n",
    "            n = 2\n",
    "            out = out * torch.exp(-distance[sub_edge_idx] ** n / sigma ** n / 2).view(-1, 1)\n",
    "\n",
    "        out = scatter_add(out, sub_index, dim=0)\n",
    "        if self.normalization == 'BatchNorm':\n",
    "            out = self.bn(out)\n",
    "        out = out.reshape(num_edge, 2, -1)\n",
    "        if output_final_layer_neuron != '':\n",
    "            final_layer_neuron = torch.cat([out[:, 0, :], out[:, 1, :], edge_fea], dim=-1).detach().cpu().numpy()\n",
    "            np.save(os.path.join(output_final_layer_neuron, 'final_layer_neuron.npy'), final_layer_neuron)\n",
    "        out = self.e_lin(torch.cat([out[:, 0, :], out[:, 1, :], edge_fea], dim=-1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLinear(nn.Module):\n",
    "    def __init__(self, num_linear: int, in_fea_len: int, out_fea_len: int, bias: bool = True) -> None:\n",
    "        super(MultipleLinear, self).__init__()\n",
    "        self.num_linear = num_linear\n",
    "        self.out_fea_len = out_fea_len\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_linear, in_fea_len, out_fea_len))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_linear, out_fea_len))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        # self.ln = LayerNorm(num_linear * out_fea_len)\n",
    "        # self.gn = GraphNorm(out_fea_len)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, batch_edge: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.matmul(input, self.weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias[:, None, :]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN(nn.Module):\n",
    "    def __init__(self, num_species, in_atom_fea_len, in_edge_fea_len, num_orbital,\n",
    "                 distance_expansion, gauss_stop, if_exp, if_MultipleLinear, if_edge_update, if_lcmp,\n",
    "                 normalization, atom_update_net, separate_onsite,\n",
    "                 trainable_gaussians, type_affine, num_l=5):\n",
    "        super(HGNN, self).__init__()\n",
    "        self.num_species = num_species\n",
    "        self.embed = nn.Embedding(num_species + 5, in_atom_fea_len)\n",
    "\n",
    "        # pair-type aware affine\n",
    "        if type_affine:\n",
    "            self.type_affine = nn.Embedding(\n",
    "                num_species ** 2, 2,\n",
    "                _weight=torch.stack([torch.ones(num_species ** 2), torch.zeros(num_species ** 2)], dim=-1)\n",
    "            )\n",
    "        else:\n",
    "            self.type_affine = None\n",
    "\n",
    "        if if_edge_update or (if_edge_update is False and if_lcmp is False):\n",
    "            distance_expansion_len = in_edge_fea_len\n",
    "        else:\n",
    "            distance_expansion_len = in_edge_fea_len - num_l ** 2\n",
    "        if distance_expansion == 'GaussianBasis':\n",
    "            self.distance_expansion = GaussianBasis(\n",
    "                0.0, gauss_stop, distance_expansion_len, trainable=trainable_gaussians\n",
    "            )\n",
    "\n",
    "        self.if_MultipleLinear = if_MultipleLinear\n",
    "        self.if_edge_update = if_edge_update\n",
    "        self.if_lcmp = if_lcmp\n",
    "        self.atom_update_net = atom_update_net\n",
    "        self.separate_onsite = separate_onsite\n",
    "\n",
    "        if if_lcmp == True:\n",
    "            mp_output_edge_fea_len = in_edge_fea_len - num_l ** 2\n",
    "        else:\n",
    "            assert if_MultipleLinear == False\n",
    "            mp_output_edge_fea_len = in_edge_fea_len\n",
    "\n",
    "        if if_edge_update == True:\n",
    "            self.mp1 = MPLayer(in_atom_fea_len, in_edge_fea_len, in_edge_fea_len, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp2 = MPLayer(in_atom_fea_len, in_edge_fea_len, in_edge_fea_len, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp3 = MPLayer(in_atom_fea_len, in_edge_fea_len, in_edge_fea_len, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp4 = MPLayer(in_atom_fea_len, in_edge_fea_len, in_edge_fea_len, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp5 = MPLayer(in_atom_fea_len, in_edge_fea_len, mp_output_edge_fea_len, if_exp, if_edge_update,\n",
    "                               normalization, atom_update_net, gauss_stop)\n",
    "        else:\n",
    "            self.mp1 = MPLayer(in_atom_fea_len, distance_expansion_len, None, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp2 = MPLayer(in_atom_fea_len, distance_expansion_len, None, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp3 = MPLayer(in_atom_fea_len, distance_expansion_len, None, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp4 = MPLayer(in_atom_fea_len, distance_expansion_len, None, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "            self.mp5 = MPLayer(in_atom_fea_len, distance_expansion_len, None, if_exp, if_edge_update, normalization,\n",
    "                               atom_update_net, gauss_stop)\n",
    "\n",
    "        if if_lcmp == True:\n",
    "            if self.if_MultipleLinear == True:\n",
    "                self.lcmp = LCMPLayer(in_atom_fea_len, in_edge_fea_len, 32, num_l, if_exp=if_exp)\n",
    "                self.multiple_linear1 = MultipleLinear(num_orbital, 32, 16)\n",
    "                self.multiple_linear2 = MultipleLinear(num_orbital, 16, 1)\n",
    "            else:\n",
    "                self.lcmp = LCMPLayer(in_atom_fea_len, in_edge_fea_len, num_orbital, num_l, if_exp=if_exp)\n",
    "        else:\n",
    "            self.mp_output = MPLayer(in_atom_fea_len, in_edge_fea_len, num_orbital, if_exp, if_edge_update=True,\n",
    "                                     normalization=normalization, atom_update_net=atom_update_net,\n",
    "                                     gauss_stop=gauss_stop, output_layer=True)\n",
    "\n",
    "\n",
    "    def forward(self, atom_attr, edge_idx, edge_attr, batch,\n",
    "                sub_atom_idx=None, sub_edge_idx=None, sub_edge_ang=None, sub_index=None,\n",
    "                huge_structure=False, output_final_layer_neuron=''):\n",
    "        batch_edge = batch[edge_idx[0]]\n",
    "        atom_fea0 = self.embed(atom_attr)\n",
    "        distance = edge_attr[:, 0]\n",
    "        edge_vec = edge_attr[:, 1:4] - edge_attr[:, 4:7]\n",
    "        if self.type_affine is None:\n",
    "            edge_fea0 = self.distance_expansion(distance)\n",
    "        else:\n",
    "            affine_coeff = self.type_affine(self.num_species * atom_attr[edge_idx[0]] + atom_attr[edge_idx[1]])\n",
    "            edge_fea0 = self.distance_expansion(distance * affine_coeff[:, 0] + affine_coeff[:, 1])\n",
    "\n",
    "        if self.if_edge_update == True:\n",
    "            atom_fea, edge_fea = self.mp1(atom_fea0, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "            atom_fea, edge_fea = self.mp2(atom_fea, edge_idx, edge_fea, batch, distance, edge_vec)\n",
    "            atom_fea0, edge_fea0 = atom_fea0 + atom_fea, edge_fea0 + edge_fea\n",
    "            atom_fea, edge_fea = self.mp3(atom_fea0, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "            atom_fea, edge_fea = self.mp4(atom_fea, edge_idx, edge_fea, batch, distance, edge_vec)\n",
    "            atom_fea0, edge_fea0 = atom_fea0 + atom_fea, edge_fea0 + edge_fea\n",
    "            atom_fea, edge_fea = self.mp5(atom_fea0, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "\n",
    "            if self.if_lcmp == True:\n",
    "                if self.atom_update_net == 'PAINN':\n",
    "                    atom_fea_s = atom_fea.node_fea_s\n",
    "                else:\n",
    "                    atom_fea_s = atom_fea\n",
    "                out = self.lcmp(atom_fea_s, edge_fea, sub_atom_idx, sub_edge_idx, sub_edge_ang, sub_index, distance,\n",
    "                                huge_structure, output_final_layer_neuron)\n",
    "            else:\n",
    "                atom_fea, edge_fea = self.mp_output(atom_fea, edge_idx, edge_fea, batch, distance, edge_vec)\n",
    "                out = edge_fea\n",
    "        else:\n",
    "            atom_fea = self.mp1(atom_fea0, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "            atom_fea = self.mp2(atom_fea, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "            atom_fea0 = atom_fea0 + atom_fea\n",
    "            atom_fea = self.mp3(atom_fea0, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "            atom_fea = self.mp4(atom_fea, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "            atom_fea0 = atom_fea0 + atom_fea\n",
    "            atom_fea = self.mp5(atom_fea0, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "\n",
    "            if self.atom_update_net == 'PAINN':\n",
    "                atom_fea_s = atom_fea.node_fea_s\n",
    "            else:\n",
    "                atom_fea_s = atom_fea\n",
    "            if self.if_lcmp == True:\n",
    "                out = self.lcmp(atom_fea_s, edge_fea0, sub_atom_idx, sub_edge_idx, sub_edge_ang, sub_index, distance,\n",
    "                                huge_structure, output_final_layer_neuron)\n",
    "            else:\n",
    "                atom_fea, edge_fea = self.mp_output(atom_fea, edge_idx, edge_fea0, batch, distance, edge_vec)\n",
    "                out = edge_fea\n",
    "\n",
    "        if self.if_MultipleLinear == True:\n",
    "            out = self.multiple_linear1(F.silu(out), batch_edge)\n",
    "            out = self.multiple_linear2(F.silu(out), batch_edge)\n",
    "            out = out.T\n",
    "\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data file: HGraph-h5-Bi_soc-5l-FromDFT.pkl\n",
      "Use existing graph data file\n",
      "Atomic types: [83]\n",
      "Finish loading the processed 82 structures (spinful: True, the number of atomic types: 1), cost 1 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = HData(\n",
    "    raw_data_dir='/home/t.hsu/example2/work_dir/dataset/processed',\n",
    "    graph_dir='/home/t.hsu/example2/work_dir/dataset/graph',\n",
    "    interface='h5',\n",
    "    target='hamiltonian',\n",
    "    dataset_name='Bi_soc',\n",
    "    multiprocessing=0,\n",
    "    radius='-1.0',\n",
    "    max_num_nbr=0,\n",
    "    num_l='5',\n",
    "    max_element='-1',\n",
    "    create_from_DFT='True',\n",
    "    if_lcmp_graph='True',\n",
    "    separate_onsite='False',\n",
    "    new_sp=False,\n",
    "    default_dtype_torch=torch.get_default_dtype(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t.hsu/miniconda3/envs/gpu_test/lib/python3.10/site-packages/deeph/kernel.py:53: UserWarning: Unable to copy scripts\n",
      "  warnings.warn(\"Unable to copy scripts\")\n",
      "====== CONFIG ======\n",
      "[basic]\n",
      "graph_dir=/home/t.hsu/example2/work_dir/dataset/graph\n",
      "save_dir=/home/t.hsu/example2/work_dir/trained_model\n",
      "raw_dir=/home/t.hsu/example2/work_dir/dataset/processed\n",
      "dataset_name=Bi_soc\n",
      "only_get_graph=False\n",
      "interface=h5\n",
      "target=hamiltonian\n",
      "disable_cuda=False\n",
      "device=cuda:0\n",
      "num_threads=64\n",
      "save_to_time_folder=False\n",
      "save_csv=False\n",
      "tb_writer=True\n",
      "seed=42\n",
      "multiprocessing=0\n",
      "orbital=[{\"83 83\": [0, 0]}, {\"83 83\": [0, 1]}, {\"83 83\": [0, 2]}, {\"83 83\": [0, 3]}, {\"83 83\": [0, 4]}, {\"83 83\": [0, 5]}, {\"83 83\": [0, 6]}, {\"83 83\": [0, 7]}, {\"83 83\": [0, 8]}, {\"83 83\": [0, 9]}, {\"83 83\": [0, 10]}, {\"83 83\": [0, 11]}, {\"83 83\": [0, 12]}, {\"83 83\": [0, 13]}, {\"83 83\": [0, 14]}, {\"83 83\": [0, 15]}, {\"83 83\": [0, 16]}, {\"83 83\": [0, 17]}, {\"83 83\": [0, 18]}, {\"83 83\": [1, 0]}, {\"83 83\": [1, 1]}, {\"83 83\": [1, 2]}, {\"83 83\": [1, 3]}, {\"83 83\": [1, 4]}, {\"83 83\": [1, 5]}, {\"83 83\": [1, 6]}, {\"83 83\": [1, 7]}, {\"83 83\": [1, 8]}, {\"83 83\": [1, 9]}, {\"83 83\": [1, 10]}, {\"83 83\": [1, 11]}, {\"83 83\": [1, 12]}, {\"83 83\": [1, 13]}, {\"83 83\": [1, 14]}, {\"83 83\": [1, 15]}, {\"83 83\": [1, 16]}, {\"83 83\": [1, 17]}, {\"83 83\": [1, 18]}, {\"83 83\": [2, 0]}, {\"83 83\": [2, 1]}, {\"83 83\": [2, 2]}, {\"83 83\": [2, 3]}, {\"83 83\": [2, 4]}, {\"83 83\": [2, 5]}, {\"83 83\": [2, 6]}, {\"83 83\": [2, 7]}, {\"83 83\": [2, 8]}, {\"83 83\": [2, 9]}, {\"83 83\": [2, 10]}, {\"83 83\": [2, 11]}, {\"83 83\": [2, 12]}, {\"83 83\": [2, 13]}, {\"83 83\": [2, 14]}, {\"83 83\": [2, 15]}, {\"83 83\": [2, 16]}, {\"83 83\": [2, 17]}, {\"83 83\": [2, 18]}, {\"83 83\": [3, 0]}, {\"83 83\": [3, 1]}, {\"83 83\": [3, 2]}, {\"83 83\": [3, 3]}, {\"83 83\": [3, 4]}, {\"83 83\": [3, 5]}, {\"83 83\": [3, 6]}, {\"83 83\": [3, 7]}, {\"83 83\": [3, 8]}, {\"83 83\": [3, 9]}, {\"83 83\": [3, 10]}, {\"83 83\": [3, 11]}, {\"83 83\": [3, 12]}, {\"83 83\": [3, 13]}, {\"83 83\": [3, 14]}, {\"83 83\": [3, 15]}, {\"83 83\": [3, 16]}, {\"83 83\": [3, 17]}, {\"83 83\": [3, 18]}, {\"83 83\": [4, 0]}, {\"83 83\": [4, 1]}, {\"83 83\": [4, 2]}, {\"83 83\": [4, 3]}, {\"83 83\": [4, 4]}, {\"83 83\": [4, 5]}, {\"83 83\": [4, 6]}, {\"83 83\": [4, 7]}, {\"83 83\": [4, 8]}, {\"83 83\": [4, 9]}, {\"83 83\": [4, 10]}, {\"83 83\": [4, 11]}, {\"83 83\": [4, 12]}, {\"83 83\": [4, 13]}, {\"83 83\": [4, 14]}, {\"83 83\": [4, 15]}, {\"83 83\": [4, 16]}, {\"83 83\": [4, 17]}, {\"83 83\": [4, 18]}, {\"83 83\": [5, 0]}, {\"83 83\": [5, 1]}, {\"83 83\": [5, 2]}, {\"83 83\": [5, 3]}, {\"83 83\": [5, 4]}, {\"83 83\": [5, 5]}, {\"83 83\": [5, 6]}, {\"83 83\": [5, 7]}, {\"83 83\": [5, 8]}, {\"83 83\": [5, 9]}, {\"83 83\": [5, 10]}, {\"83 83\": [5, 11]}, {\"83 83\": [5, 12]}, {\"83 83\": [5, 13]}, {\"83 83\": [5, 14]}, {\"83 83\": [5, 15]}, {\"83 83\": [5, 16]}, {\"83 83\": [5, 17]}, {\"83 83\": [5, 18]}, {\"83 83\": [6, 0]}, {\"83 83\": [6, 1]}, {\"83 83\": [6, 2]}, {\"83 83\": [6, 3]}, {\"83 83\": [6, 4]}, {\"83 83\": [6, 5]}, {\"83 83\": [6, 6]}, {\"83 83\": [6, 7]}, {\"83 83\": [6, 8]}, {\"83 83\": [6, 9]}, {\"83 83\": [6, 10]}, {\"83 83\": [6, 11]}, {\"83 83\": [6, 12]}, {\"83 83\": [6, 13]}, {\"83 83\": [6, 14]}, {\"83 83\": [6, 15]}, {\"83 83\": [6, 16]}, {\"83 83\": [6, 17]}, {\"83 83\": [6, 18]}, {\"83 83\": [7, 0]}, {\"83 83\": [7, 1]}, {\"83 83\": [7, 2]}, {\"83 83\": [7, 3]}, {\"83 83\": [7, 4]}, {\"83 83\": [7, 5]}, {\"83 83\": [7, 6]}, {\"83 83\": [7, 7]}, {\"83 83\": [7, 8]}, {\"83 83\": [7, 9]}, {\"83 83\": [7, 10]}, {\"83 83\": [7, 11]}, {\"83 83\": [7, 12]}, {\"83 83\": [7, 13]}, {\"83 83\": [7, 14]}, {\"83 83\": [7, 15]}, {\"83 83\": [7, 16]}, {\"83 83\": [7, 17]}, {\"83 83\": [7, 18]}, {\"83 83\": [8, 0]}, {\"83 83\": [8, 1]}, {\"83 83\": [8, 2]}, {\"83 83\": [8, 3]}, {\"83 83\": [8, 4]}, {\"83 83\": [8, 5]}, {\"83 83\": [8, 6]}, {\"83 83\": [8, 7]}, {\"83 83\": [8, 8]}, {\"83 83\": [8, 9]}, {\"83 83\": [8, 10]}, {\"83 83\": [8, 11]}, {\"83 83\": [8, 12]}, {\"83 83\": [8, 13]}, {\"83 83\": [8, 14]}, {\"83 83\": [8, 15]}, {\"83 83\": [8, 16]}, {\"83 83\": [8, 17]}, {\"83 83\": [8, 18]}, {\"83 83\": [9, 0]}, {\"83 83\": [9, 1]}, {\"83 83\": [9, 2]}, {\"83 83\": [9, 3]}, {\"83 83\": [9, 4]}, {\"83 83\": [9, 5]}, {\"83 83\": [9, 6]}, {\"83 83\": [9, 7]}, {\"83 83\": [9, 8]}, {\"83 83\": [9, 9]}, {\"83 83\": [9, 10]}, {\"83 83\": [9, 11]}, {\"83 83\": [9, 12]}, {\"83 83\": [9, 13]}, {\"83 83\": [9, 14]}, {\"83 83\": [9, 15]}, {\"83 83\": [9, 16]}, {\"83 83\": [9, 17]}, {\"83 83\": [9, 18]}, {\"83 83\": [10, 0]}, {\"83 83\": [10, 1]}, {\"83 83\": [10, 2]}, {\"83 83\": [10, 3]}, {\"83 83\": [10, 4]}, {\"83 83\": [10, 5]}, {\"83 83\": [10, 6]}, {\"83 83\": [10, 7]}, {\"83 83\": [10, 8]}, {\"83 83\": [10, 9]}, {\"83 83\": [10, 10]}, {\"83 83\": [10, 11]}, {\"83 83\": [10, 12]}, {\"83 83\": [10, 13]}, {\"83 83\": [10, 14]}, {\"83 83\": [10, 15]}, {\"83 83\": [10, 16]}, {\"83 83\": [10, 17]}, {\"83 83\": [10, 18]}, {\"83 83\": [11, 0]}, {\"83 83\": [11, 1]}, {\"83 83\": [11, 2]}, {\"83 83\": [11, 3]}, {\"83 83\": [11, 4]}, {\"83 83\": [11, 5]}, {\"83 83\": [11, 6]}, {\"83 83\": [11, 7]}, {\"83 83\": [11, 8]}, {\"83 83\": [11, 9]}, {\"83 83\": [11, 10]}, {\"83 83\": [11, 11]}, {\"83 83\": [11, 12]}, {\"83 83\": [11, 13]}, {\"83 83\": [11, 14]}, {\"83 83\": [11, 15]}, {\"83 83\": [11, 16]}, {\"83 83\": [11, 17]}, {\"83 83\": [11, 18]}, {\"83 83\": [12, 0]}, {\"83 83\": [12, 1]}, {\"83 83\": [12, 2]}, {\"83 83\": [12, 3]}, {\"83 83\": [12, 4]}, {\"83 83\": [12, 5]}, {\"83 83\": [12, 6]}, {\"83 83\": [12, 7]}, {\"83 83\": [12, 8]}, {\"83 83\": [12, 9]}, {\"83 83\": [12, 10]}, {\"83 83\": [12, 11]}, {\"83 83\": [12, 12]}, {\"83 83\": [12, 13]}, {\"83 83\": [12, 14]}, {\"83 83\": [12, 15]}, {\"83 83\": [12, 16]}, {\"83 83\": [12, 17]}, {\"83 83\": [12, 18]}, {\"83 83\": [13, 0]}, {\"83 83\": [13, 1]}, {\"83 83\": [13, 2]}, {\"83 83\": [13, 3]}, {\"83 83\": [13, 4]}, {\"83 83\": [13, 5]}, {\"83 83\": [13, 6]}, {\"83 83\": [13, 7]}, {\"83 83\": [13, 8]}, {\"83 83\": [13, 9]}, {\"83 83\": [13, 10]}, {\"83 83\": [13, 11]}, {\"83 83\": [13, 12]}, {\"83 83\": [13, 13]}, {\"83 83\": [13, 14]}, {\"83 83\": [13, 15]}, {\"83 83\": [13, 16]}, {\"83 83\": [13, 17]}, {\"83 83\": [13, 18]}, {\"83 83\": [14, 0]}, {\"83 83\": [14, 1]}, {\"83 83\": [14, 2]}, {\"83 83\": [14, 3]}, {\"83 83\": [14, 4]}, {\"83 83\": [14, 5]}, {\"83 83\": [14, 6]}, {\"83 83\": [14, 7]}, {\"83 83\": [14, 8]}, {\"83 83\": [14, 9]}, {\"83 83\": [14, 10]}, {\"83 83\": [14, 11]}, {\"83 83\": [14, 12]}, {\"83 83\": [14, 13]}, {\"83 83\": [14, 14]}, {\"83 83\": [14, 15]}, {\"83 83\": [14, 16]}, {\"83 83\": [14, 17]}, {\"83 83\": [14, 18]}, {\"83 83\": [15, 0]}, {\"83 83\": [15, 1]}, {\"83 83\": [15, 2]}, {\"83 83\": [15, 3]}, {\"83 83\": [15, 4]}, {\"83 83\": [15, 5]}, {\"83 83\": [15, 6]}, {\"83 83\": [15, 7]}, {\"83 83\": [15, 8]}, {\"83 83\": [15, 9]}, {\"83 83\": [15, 10]}, {\"83 83\": [15, 11]}, {\"83 83\": [15, 12]}, {\"83 83\": [15, 13]}, {\"83 83\": [15, 14]}, {\"83 83\": [15, 15]}, {\"83 83\": [15, 16]}, {\"83 83\": [15, 17]}, {\"83 83\": [15, 18]}, {\"83 83\": [16, 0]}, {\"83 83\": [16, 1]}, {\"83 83\": [16, 2]}, {\"83 83\": [16, 3]}, {\"83 83\": [16, 4]}, {\"83 83\": [16, 5]}, {\"83 83\": [16, 6]}, {\"83 83\": [16, 7]}, {\"83 83\": [16, 8]}, {\"83 83\": [16, 9]}, {\"83 83\": [16, 10]}, {\"83 83\": [16, 11]}, {\"83 83\": [16, 12]}, {\"83 83\": [16, 13]}, {\"83 83\": [16, 14]}, {\"83 83\": [16, 15]}, {\"83 83\": [16, 16]}, {\"83 83\": [16, 17]}, {\"83 83\": [16, 18]}, {\"83 83\": [17, 0]}, {\"83 83\": [17, 1]}, {\"83 83\": [17, 2]}, {\"83 83\": [17, 3]}, {\"83 83\": [17, 4]}, {\"83 83\": [17, 5]}, {\"83 83\": [17, 6]}, {\"83 83\": [17, 7]}, {\"83 83\": [17, 8]}, {\"83 83\": [17, 9]}, {\"83 83\": [17, 10]}, {\"83 83\": [17, 11]}, {\"83 83\": [17, 12]}, {\"83 83\": [17, 13]}, {\"83 83\": [17, 14]}, {\"83 83\": [17, 15]}, {\"83 83\": [17, 16]}, {\"83 83\": [17, 17]}, {\"83 83\": [17, 18]}, {\"83 83\": [18, 0]}, {\"83 83\": [18, 1]}, {\"83 83\": [18, 2]}, {\"83 83\": [18, 3]}, {\"83 83\": [18, 4]}, {\"83 83\": [18, 5]}, {\"83 83\": [18, 6]}, {\"83 83\": [18, 7]}, {\"83 83\": [18, 8]}, {\"83 83\": [18, 9]}, {\"83 83\": [18, 10]}, {\"83 83\": [18, 11]}, {\"83 83\": [18, 12]}, {\"83 83\": [18, 13]}, {\"83 83\": [18, 14]}, {\"83 83\": [18, 15]}, {\"83 83\": [18, 16]}, {\"83 83\": [18, 17]}, {\"83 83\": [18, 18]}]\n",
      "o_component=H\n",
      "energy_component=summation\n",
      "max_element=-1\n",
      "statistics=False\n",
      "normalizer=False\n",
      "boxcox=False\n",
      "\n",
      "[graph]\n",
      "radius=-1.0\n",
      "max_num_nbr=0\n",
      "create_from_dft=True\n",
      "if_lcmp_graph=True\n",
      "separate_onsite=False\n",
      "new_sp=False\n",
      "\n",
      "[train]\n",
      "epochs=2\n",
      "pretrained=\n",
      "resume=\n",
      "train_ratio=0.6\n",
      "val_ratio=0.2\n",
      "test_ratio=0.2\n",
      "early_stopping_loss=0.0\n",
      "early_stopping_loss_epoch=[0.000000, 500]\n",
      "revert_then_decay=True\n",
      "revert_threshold=30\n",
      "revert_decay_epoch=[250, 2500]\n",
      "revert_decay_gamma=[0.2, 0.5]\n",
      "clip_grad=True\n",
      "clip_grad_value=4.2\n",
      "switch_sgd=False\n",
      "switch_sgd_lr=1e-4\n",
      "switch_sgd_epoch=-1\n",
      "\n",
      "[hyperparameter]\n",
      "batch_size=1\n",
      "dtype=float32\n",
      "optimizer=adam\n",
      "learning_rate=0.001\n",
      "lr_scheduler=\n",
      "lr_milestones=[]\n",
      "momentum=0.9\n",
      "weight_decay=0\n",
      "criterion=MaskMSELoss\n",
      "retain_edge_fea=True\n",
      "lambda_eij=0.0\n",
      "lambda_ei=0.1\n",
      "lambda_etot=0.0\n",
      "\n",
      "[network]\n",
      "atom_fea_len=64\n",
      "edge_fea_len=128\n",
      "gauss_stop=8.5\n",
      "num_l=5\n",
      "aggr=add\n",
      "distance_expansion=GaussianBasis\n",
      "if_exp=True\n",
      "if_multiplelinear=False\n",
      "if_edge_update=True\n",
      "if_lcmp=True\n",
      "normalization=LayerNorm\n",
      "atom_update_net=CGConv\n",
      "trainable_gaussians=False\n",
      "type_affine=False\n",
      "\n",
      "Graph data file: HGraph-h5-Bi_soc-5l-FromDFT.pkl\n",
      "Use existing graph data file\n",
      "Atomic types: [83]\n",
      "Finish loading the processed 82 structures (spinful: True, the number of atomic types: 1), cost 1 seconds\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import json\n",
    "from deeph.kernel import DeepHKernel\n",
    "# Initialize the ConfigParser\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the configuration file\n",
    "config.read('/home/t.hsu/deeph_dev/default.ini')\n",
    "\n",
    "config.read('/home/t.hsu/deeph_dev/train.ini')  # Replace 'config.ini' with your actual file name\n",
    "num_species = len(dataset.info[\"index_to_Z\"])\n",
    "orbital = json.loads(config.get('basic', 'orbital'))\n",
    "num_orbital = len(orbital)\n",
    "spinful = True\n",
    "if_lcmp = True\n",
    "separate_onsite = False\n",
    "if spinful:\n",
    "    out_fea_len = num_orbital * 8\n",
    "else:\n",
    "    out_fea_len = num_orbital\n",
    "print('DeepH', out_fea_len)\n",
    "out_fea_len = 999\n",
    "print('DOS', out_fea_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    n_elements=num_species,\n",
    "    num_species=num_species,\n",
    "    in_atom_fea_len=config.getint('network', 'atom_fea_len'),\n",
    "    in_vfeats=config.getint('network', 'atom_fea_len'),\n",
    "    in_edge_fea_len=config.getint('network', 'edge_fea_len'),\n",
    "    in_efeats=config.getint('network', 'edge_fea_len'),\n",
    "    out_edge_fea_len=out_fea_len,\n",
    "    out_efeats=out_fea_len,\n",
    "    num_orbital=out_fea_len,\n",
    "    distance_expansion=config.get('network', 'distance_expansion'),\n",
    "    gauss_stop=config.getfloat('network', 'gauss_stop'),\n",
    "    cutoff=config.getfloat('network', 'gauss_stop'),\n",
    "    if_exp=config.getboolean('network', 'if_exp'),\n",
    "    if_MultipleLinear=config.getboolean('network', 'if_MultipleLinear'),\n",
    "    if_edge_update=config.getboolean('network', 'if_edge_update'),\n",
    "    if_lcmp=if_lcmp,\n",
    "    normalization=config.get('network', 'normalization'),\n",
    "    atom_update_net=config.get('network', 'atom_update_net', fallback='CGConv'),\n",
    "    separate_onsite=separate_onsite,\n",
    "    num_l=config.getint('network', 'num_l'),\n",
    "    trainable_gaussians=config.getboolean('network', 'trainable_gaussians', fallback=False),\n",
    "    type_affine=config.getboolean('network', 'type_affine', fallback=False),\n",
    "    if_fc_out=False,\n",
    ")\n",
    "parameter_list = list(signature(HGNN.__init__).parameters.keys())\n",
    "current_parameter_list = list(model_kwargs.keys())\n",
    "for k in current_parameter_list:\n",
    "    if k not in parameter_list:\n",
    "        model_kwargs.pop(k)\n",
    "if 'num_elements' in parameter_list:\n",
    "    model_kwargs['num_elements'] = config.getint('basic', 'max_element') + 1\n",
    "model = HGNN(**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"The model you built has: %d parameters\" % params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train set: 49\n",
      "number of val set: 16\n",
      "number of test set: 16\n",
      "{'normalizer': False, 'boxcox': False}\n",
      "DeepH 2888\n",
      "DOS 999\n",
      "The model you built has: 600910 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(config.get('basic', 'device') if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = LossRecord()\n",
    "model.train()\n",
    "for step, batch_tuple in enumerate(train_loader):\n",
    "    if if_lcmp:\n",
    "        batch, subgraph = batch_tuple\n",
    "        sub_atom_idx, sub_edge_idx, sub_edge_ang, sub_index = subgraph\n",
    "        output = model(\n",
    "            batch.x.to(device),\n",
    "            batch.edge_index.to(device),\n",
    "            batch.edge_attr.to(device),\n",
    "            batch.batch.to(device),\n",
    "            sub_atom_idx.to(device),\n",
    "            sub_edge_idx.to(device),\n",
    "            sub_edge_ang.to(device),\n",
    "            sub_index.to(device)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1518, 2888])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minideeph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
